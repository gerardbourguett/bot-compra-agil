from curl_cffi import requests
import database_extended as db  # Usamos la base de datos extendida
from datetime import datetime, timedelta
import time

# ConfiguraciÃ³n de la API
API_BASE_URL = "https://api.buscador.mercadopublico.cl/compra-agil"
API_KEY = "e93089e4-437c-4723-b343-4fa20045e3bc"


def obtener_headers():
    """
    Construye los headers necesarios para las peticiones a la API.
    Solo necesitamos la X-API-Key, no se requiere token Bearer.
    """
    return {
        "accept": "application/json, text/plain, */*",
        "accept-language": "es-ES,es;q=0.9",
        "origin": "https://buscador.mercadopublico.cl",
        "referer": "https://buscador.mercadopublico.cl/",
        "sec-ch-ua": '"Chromium";v="142", "Google Chrome";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-site",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
        "x-api-key": API_KEY
    }


def obtener_licitaciones(date_from, date_to, status=2, page_number=1):
    """
    Obtiene las licitaciones de la API para una pÃ¡gina especÃ­fica.
    
    Args:
        date_from: Fecha inicial (formato: YYYY-MM-DD)
        date_to: Fecha final (formato: YYYY-MM-DD)
        status: Estado de las licitaciones (2 = Publicada)
        page_number: NÃºmero de pÃ¡gina a obtener
    
    Returns:
        dict: Respuesta JSON de la API o None si hay error
    """
    params = {
        "date_from": date_from,
        "date_to": date_to,
        "order_by": "recent",
        "page_number": page_number,
        "status": status
    }
    
    headers = obtener_headers()
    
    try:
        response = requests.get(API_BASE_URL, params=params, headers=headers, impersonate="chrome120")
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ Error al obtener licitaciones (pÃ¡gina {page_number}): {e}")
        return None


def ejecutar_scraper(dias_atras=30, max_paginas=None):
    """
    Ejecuta el scraper completo obteniendo todas las pÃ¡ginas de resultados.
    
    Args:
        dias_atras: NÃºmero de dÃ­as hacia atrÃ¡s para buscar licitaciones (default: 30)
        max_paginas: NÃºmero mÃ¡ximo de pÃ¡ginas a procesar (None = todas)
    """
    print("ğŸ•·ï¸ Iniciando Scraper de Compra Ãgil...")
    db.iniciar_db_extendida()  # Aseguramos que la tabla exista
    
    # Calcular fechas
    fecha_hasta = datetime.now()
    fecha_desde = fecha_hasta - timedelta(days=dias_atras)
    
    date_from = fecha_desde.strftime("%Y-%m-%d")
    date_to = fecha_hasta.strftime("%Y-%m-%d")
    
    print(f"ğŸ“… Buscando licitaciones desde {date_from} hasta {date_to}")
    
    nuevos_total = 0
    page_number = 1
    
    while True:
        # Verificar si alcanzamos el mÃ¡ximo de pÃ¡ginas
        if max_paginas and page_number > max_paginas:
            print(f"âœ… Alcanzado el lÃ­mite de {max_paginas} pÃ¡ginas")
            break
        
        print(f"\nğŸ“„ Procesando pÃ¡gina {page_number}...")
        
        data = obtener_licitaciones(date_from, date_to, status=2, page_number=page_number)
        
        if not data or data.get('success') != 'OK':
            print("âŒ Error en la respuesta de la API")
            break
        
        payload = data.get('payload', {})
        items = payload.get('resultados', [])
        
        if not items:
            print("âœ… No hay mÃ¡s resultados")
            break
        
        # Mostrar informaciÃ³n de progreso
        if page_number == 1:
            total_resultados = payload.get('resultCount', 0)
            total_paginas = payload.get('pageCount', 0)
            print(f"ğŸ“Š Total de licitaciones encontradas: {total_resultados}")
            print(f"ğŸ“„ Total de pÃ¡ginas: {total_paginas}")
        
        print(f"   Procesando {len(items)} licitaciones...")
        
        for item in items:
            datos_tupla = (
                item.get('id'),
                item.get('codigo'),
                item.get('nombre'),
                item.get('fecha_publicacion'),
                item.get('fecha_cierre'),
                item.get('organismo'),
                item.get('unidad'),
                item.get('id_estado'),
                item.get('estado'),
                item.get('monto_disponible'),
                item.get('moneda'),
                item.get('monto_disponible_CLP'),
                item.get('fecha_cambio'),
                item.get('valor_cambio_moneda'),
                item.get('cantidad_proveedores_cotizando'),
                item.get('estado_convocatoria')
            )
            nuevos_total += db.guardar_licitacion_basica(datos_tupla)
        
        # Verificar si hay mÃ¡s pÃ¡ginas
        page_count = payload.get('pageCount', 0)
        if page_number >= page_count:
            print(f"âœ… Todas las pÃ¡ginas procesadas ({page_count} pÃ¡ginas)")
            break
        
        page_number += 1

    """
    Ejecuta el scraper completo obteniendo todas las pÃ¡ginas de resultados.
    
    Args:
        dias_atras: NÃºmero de dÃ­as hacia atrÃ¡s para buscar licitaciones (default: 30)
        max_paginas: NÃºmero mÃ¡ximo de pÃ¡ginas a procesar (None = todas)
    """
    print("ğŸ•·ï¸ Iniciando Scraper de Compra Ãgil...")
    db.iniciar_db_extendida()  # Aseguramos que la tabla exista
    
    # Calcular fechas
    fecha_hasta = datetime.now()
    fecha_desde = fecha_hasta - timedelta(days=dias_atras)
    
    date_from = fecha_desde.strftime("%Y-%m-%d")
    date_to = fecha_hasta.strftime("%Y-%m-%d")
    
    print(f"ğŸ“… Buscando licitaciones desde {date_from} hasta {date_to}")
    
    nuevos_total = 0
    page_number = 1
    
    while True:
        # Verificar si alcanzamos el mÃ¡ximo de pÃ¡ginas
        if max_paginas and page_number > max_paginas:
            print(f"âœ… Alcanzado el lÃ­mite de {max_paginas} pÃ¡ginas")
            break
        
        print(f"\nğŸ“„ Procesando pÃ¡gina {page_number}...")
        
        data = obtener_licitaciones(date_from, date_to, status=2, page_number=page_number)
        
        if not data or data.get('success') != 'OK':
            print("âŒ Error en la respuesta de la API")
            break
        
        payload = data.get('payload', {})
        items = payload.get('resultados', [])
        
        if not items:
            print("âœ… No hay mÃ¡s resultados")
            break
        
        # Mostrar informaciÃ³n de progreso
        if page_number == 1:
            total_resultados = payload.get('resultCount', 0)
            total_paginas = payload.get('pageCount', 0)
            print(f"ğŸ“Š Total de licitaciones encontradas: {total_resultados}")
            print(f"ğŸ“„ Total de pÃ¡ginas: {total_paginas}")
        
        print(f"   Procesando {len(items)} licitaciones...")
        
        for item in items:
            datos_tupla = (
                item.get('id'),
                item.get('codigo'),
                item.get('nombre'),
                item.get('fecha_publicacion'),
                item.get('fecha_cierre'),
                item.get('organismo'),
                item.get('unidad'),
                item.get('id_estado'),
                item.get('estado'),
                item.get('monto_disponible'),
                item.get('moneda'),
                item.get('monto_disponible_CLP'),
                item.get('fecha_cambio'),
                item.get('valor_cambio_moneda'),
                item.get('cantidad_proveedores_cotizando'),
                item.get('estado_convocatoria')
            )
            nuevos_total += db.guardar_licitacion_basica(datos_tupla)
        
        # Verificar si hay mÃ¡s pÃ¡ginas
        page_count = payload.get('pageCount', 0)
        if page_number >= page_count:
            print(f"âœ… Todas las pÃ¡ginas procesadas ({page_count} pÃ¡ginas)")
            break
        
        page_number += 1
        time.sleep(0.5)  # PequeÃ±a pausa entre peticiones para no sobrecargar el servidor
```python
        
        if not data or data.get('success') != 'OK':
            print("âŒ Error en la respuesta de la API")
            break
        
        payload = data.get('payload', {})
        items = payload.get('resultados', [])
        
        if not items:
            print("âœ… No hay mÃ¡s resultados")
            break
        
        # Mostrar informaciÃ³n de progreso
        if page_number == 1:
            total_resultados = payload.get('resultCount', 0)
            total_paginas = payload.get('pageCount', 0)
            print(f"ğŸ“Š Total de licitaciones encontradas: {total_resultados}")
            print(f"ğŸ“„ Total de pÃ¡ginas: {total_paginas}")
        
        print(f"   Procesando {len(items)} licitaciones...")
        
        for item in items:
            datos_tupla = (
                item.get('id'),
                item.get('codigo'),
                item.get('nombre'),
                item.get('fecha_publicacion'),
                item.get('fecha_cierre'),
                item.get('organismo'),
                item.get('unidad'),
                item.get('id_estado'),
                item.get('estado'),
                item.get('monto_disponible'),
                item.get('moneda'),
                item.get('monto_disponible_CLP'),
                item.get('fecha_cambio'),
                item.get('valor_cambio_moneda'),
                item.get('cantidad_proveedores_cotizando'),
                item.get('estado_convocatoria')
            )
            nuevos_total += db.guardar_licitacion_basica(datos_tupla)
        
        # Verificar si hay mÃ¡s pÃ¡ginas
        page_count = payload.get('pageCount', 0)
        if page_number >= page_count:
            print(f"âœ… Todas las pÃ¡ginas procesadas ({page_count} pÃ¡ginas)")
            break
        
        page_number += 1
from curl_cffi import requests
import database_extended as db  # Usamos la base de datos extendida
from datetime import datetime, timedelta
import time

# ConfiguraciÃ³n de la API
API_BASE_URL = "https://api.buscador.mercadopublico.cl/compra-agil"
API_KEY = "e93089e4-437c-4723-b343-4fa20045e3bc"


def obtener_headers():
    """
    Construye los headers necesarios para las peticiones a la API.
    Solo necesitamos la X-API-Key, no se requiere token Bearer.
    """
    return {
        "accept": "application/json, text/plain, */*",
        "accept-language": "es-ES,es;q=0.9",
        "origin": "https://buscador.mercadopublico.cl",
        "referer": "https://buscador.mercadopublico.cl/",
        "sec-ch-ua": '"Chromium";v="142", "Google Chrome";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-site",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
        "x-api-key": API_KEY
    }


def obtener_licitaciones(date_from, date_to, status=2, page_number=1):
    """
    Obtiene las licitaciones de la API para una pÃ¡gina especÃ­fica.
    
    Args:
        date_from: Fecha inicial (formato: YYYY-MM-DD)
        date_to: Fecha final (formato: YYYY-MM-DD)
        status: Estado de las licitaciones (2 = Publicada)
        page_number: NÃºmero de pÃ¡gina a obtener
    
    Returns:
        dict: Respuesta JSON de la API o None si hay error
    """
    params = {
        "date_from": date_from,
        "date_to": date_to,
        "order_by": "recent",
        "page_number": page_number,
        "status": status
    }
    
    headers = obtener_headers()
    
    try:
        response = requests.get(API_BASE_URL, params=params, headers=headers, impersonate="chrome120")
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ Error al obtener licitaciones (pÃ¡gina {page_number}): {e}")
        return None


def ejecutar_scraper(dias_atras=30, max_paginas=None):
    """
    Ejecuta el scraper completo obteniendo todas las pÃ¡ginas de resultados.
    
    Args:
        dias_atras: NÃºmero de dÃ­as hacia atrÃ¡s para buscar licitaciones (default: 30)
        max_paginas: NÃºmero mÃ¡ximo de pÃ¡ginas a procesar (None = todas)
    """
    print("ğŸ•·ï¸ Iniciando Scraper de Compra Ãgil...")
    db.iniciar_db_extendida()  # Aseguramos que la tabla exista
    
    # Calcular fechas
    fecha_hasta = datetime.now()
    fecha_desde = fecha_hasta - timedelta(days=dias_atras)
    
    date_from = fecha_desde.strftime("%Y-%m-%d")
    date_to = fecha_hasta.strftime("%Y-%m-%d")
    
    print(f"ğŸ“… Buscando licitaciones desde {date_from} hasta {date_to}")
    
    nuevos_total = 0
    page_number = 1
    
    while True:
        # Verificar si alcanzamos el mÃ¡ximo de pÃ¡ginas
        if max_paginas and page_number > max_paginas:
            print(f"âœ… Alcanzado el lÃ­mite de {max_paginas} pÃ¡ginas")
            break
        
        print(f"\nğŸ“„ Procesando pÃ¡gina {page_number}...")
        
        data = obtener_licitaciones(date_from, date_to, status=2, page_number=page_number)
        
        if not data or data.get('success') != 'OK':
            print("âŒ Error en la respuesta de la API")
            break
        
        payload = data.get('payload', {})
        items = payload.get('resultados', [])
        
        if not items:
            print("âœ… No hay mÃ¡s resultados")
            break
        
        # Mostrar informaciÃ³n de progreso
        if page_number == 1:
            total_resultados = payload.get('resultCount', 0)
            total_paginas = payload.get('pageCount', 0)
            print(f"ğŸ“Š Total de licitaciones encontradas: {total_resultados}")
            print(f"ğŸ“„ Total de pÃ¡ginas: {total_paginas}")
        
        print(f"   Procesando {len(items)} licitaciones...")
        
        for item in items:
            datos_tupla = (
                item.get('id'),
                item.get('codigo'),
                item.get('nombre'),
                item.get('fecha_publicacion'),
                item.get('fecha_cierre'),
                item.get('organismo'),
                item.get('unidad'),
                item.get('id_estado'),
                item.get('estado'),
                item.get('monto_disponible'),
                item.get('moneda'),
                item.get('monto_disponible_CLP'),
                item.get('fecha_cambio'),
                item.get('valor_cambio_moneda'),
                item.get('cantidad_proveedores_cotizando'),
                item.get('estado_convocatoria')
            )
            nuevos_total += db.guardar_licitacion_basica(datos_tupla)
        
        # Verificar si hay mÃ¡s pÃ¡ginas
        page_count = payload.get('pageCount', 0)
        if page_number >= page_count:
            print(f"âœ… Todas las pÃ¡ginas procesadas ({page_count} pÃ¡ginas)")
            break
        
        page_number += 1
        time.sleep(0.5)  # PequeÃ±a pausa entre peticiones para no sobrecargar el servidor
    
    print(f"\nâœ… Proceso terminado. Se guardaron {nuevos_total} licitaciones nuevas.")
    print(f"ğŸ“Š Total de licitaciones procesadas: {(page_number - 1) * 15 + len(items)}")


if __name__ == "__main__":
    # Por defecto busca los Ãºltimos 30 dÃ­as
    # Puedes cambiar el nÃºmero de dÃ­as o limitar las pÃ¡ginas para pruebas
    # Ejemplo: ejecutar_scraper(dias_atras=7, max_paginas=5)
    ejecutar_scraper(dias_atras=30)
```